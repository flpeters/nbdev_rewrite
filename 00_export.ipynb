{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from collections import namedtuple, defaultdict\n",
    "import os\n",
    "import re\n",
    "from nbdev.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "def run_tests(cases, func, verbose=False):\n",
    "    nr_correct = 0\n",
    "    for i, (n, c, r) in enumerate(cases):\n",
    "        if verbose: print(f'({i + 1} / {len(cases)}) TEST {n}:')\n",
    "        try:\n",
    "            res = func(c)\n",
    "            assert res == r, f'TEST FAILED WITH RESULT: {res}\\nEXPECTED: {r}'\n",
    "            nr_correct += (res == r)\n",
    "            if verbose: print(f'TEST RESULT: SUCCESS\\n')\n",
    "        except Exception as e:\n",
    "            if verbose: print(f'TEST FAILED WITH EXCEPTION:\\n{e}\\n')\n",
    "            # raise e\n",
    "    print('--------------- ALL TESTS COMPLETED ---------------')\n",
    "    print(f'{nr_correct} / {len(cases)} Correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "create_config('nbdev-rewrite', 'flpeters', nbs_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if not os.environ.get(\"IN_TEST\", None):\n",
    "    assert IN_NOTEBOOK\n",
    "    assert not IN_COLAB\n",
    "    assert IN_IPYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Notebook Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def read_nb(fname):\n",
    "    \"Read the notebook in `fname`.\"\n",
    "    with open(Path(fname),'r', encoding='utf8') as f: return nbformat.reads(f.read(), as_version=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_nb = read_nb('00_export.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cells', 'metadata', 'nbformat', 'nbformat_minor'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nb.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernelspec': {'display_name': 'Python 3',\n",
       "  'language': 'python',\n",
       "  'name': 'python3'},\n",
       " 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "  'file_extension': '.py',\n",
       "  'mimetype': 'text/x-python',\n",
       "  'name': 'python',\n",
       "  'nbconvert_exporter': 'python',\n",
       "  'pygments_lexer': 'ipython3',\n",
       "  'version': '3.7.3'},\n",
       " 'toc': {'base_numbering': 1,\n",
       "  'nav_menu': {},\n",
       "  'number_sections': True,\n",
       "  'sideBar': True,\n",
       "  'skip_h1_title': False,\n",
       "  'title_cell': 'Table of Contents',\n",
       "  'title_sidebar': 'Contents',\n",
       "  'toc_cell': False,\n",
       "  'toc_position': {},\n",
       "  'toc_section_display': True,\n",
       "  'toc_window_display': False}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nb['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{test_nb['nbformat']}.{test_nb['nbformat_minor']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell_type': 'markdown', 'metadata': {}, 'source': '# Imports'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nb['cells'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_nb['cells'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`detect_comments()` is used to find and extract all comments from a code block.  \n",
    "It's main purpose is to avoid matching on \"comments\" that are actually just part of a string, and not real python comments. One example would be: \n",
    "\n",
    "    \"\"\"\n",
    "    # export\n",
    "    \"\"\"\n",
    "A naive parser would see the literal \"#\" and match that statement. In reality however, this code snippet is a string, and might be e.g. part of a test suit (which is how this bug was found in the first place), and not really meant to be exported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This detects all comments in a piece of code, excluding those that are a part of a string.  \n",
    "View https://docs.python.org/3/reference/lexical_analysis.html#strings for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "def iter_comments(src:str, pure_comments_only:bool=True, line_limit=None):\n",
    "    in_lstr = in_sstr = False\n",
    "    count, quote = 1, ''\n",
    "    for i, line in enumerate(src.splitlines()[:line_limit]):\n",
    "        is_pure, escape, prev_c = True, False, '\\n'\n",
    "        for j, c in enumerate(line):\n",
    "            # we can't break as soon as not is_pure, because we have to detect if a multiline string beginns\n",
    "            if is_pure and (not (c.isspace() or c == '#')): is_pure = False\n",
    "            if (in_sstr or in_lstr):\n",
    "                # assert (in_sstr and not in_lstr) or (in_lstr and not in_sstr)\n",
    "                if escape: count = 0\n",
    "                else:\n",
    "                    if (c == quote):\n",
    "                        count = ((count + 1) if (c == prev_c) else 1)\n",
    "                        if in_sstr: in_sstr = False\n",
    "                        elif (in_lstr and (count == 3)): count, in_lstr = 0, False\n",
    "                escape = False if escape else (c == '\\\\')\n",
    "            else:                    \n",
    "                if (c == '#'):\n",
    "                    if (pure_comments_only and is_pure): yield (line, (i, j))\n",
    "                    elif (not pure_comments_only):       yield (line[j:], (i, j))\n",
    "                    break\n",
    "                elif c == \"'\" or c == '\"':\n",
    "                    count = ((count + 1) if (c == prev_c) else 1)\n",
    "                    if count == 1: in_sstr = True\n",
    "                    elif count == 3: count, in_lstr = 0, True\n",
    "                    else: pass#raise SyntaxError(f'Unexpected quote repetition count: {count} Should be either 1 or 3')\n",
    "                    quote = c\n",
    "            prev_c = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- ALL TESTS COMPLETED ---------------\n",
      "13 / 13 Correct\n"
     ]
    }
   ],
   "source": [
    "def test_iter_comments(src): return list(iter_comments(src, True))\n",
    "test_strings = [\n",
    "(\"trippe quote(''')\", \"\"\"'''\n",
    "# string\n",
    "'''\"\"\", []),\n",
    "('tripple quote(\"\"\")', '''\"\"\"\n",
    "#string\n",
    "\"\"\"''', []),\n",
    "('single quote(\")', '\"\\\n",
    "\\n#string\\n\\\n",
    "\"', []),\n",
    "(\"single quote(')\", \"'\\\n",
    "\\n#string\\n\\\n",
    "'\", []),\n",
    "(\"simple comment\", \"\"\"\n",
    "#comment\n",
    "\"\"\", [('#comment', (1, 0))]),\n",
    "(\"comment sandwich\", \"\"\"\n",
    "'this is a string'\n",
    "# this is a comment\n",
    "'another string , but between is an actual comment'\n",
    "\"\"\", [('# this is a comment', (2, 0))]),\n",
    "(\"tricky case 2\", \"\"\"\n",
    "  a #non-pure comment\n",
    "'''\n",
    "#string\n",
    "'''\n",
    "####comment\"\"\", [('####comment', (5, 0))]),\n",
    "(\"end of string quote\", \"\"\"\n",
    "'''\n",
    "'\n",
    "# still part of the string\n",
    "'\n",
    "'''\n",
    "\"\"\", []),\n",
    "(\"single end of string escape\", \"\"\"\n",
    "'\\\\'\\\\\\n#str\\\\\\n\\\\''\n",
    "\"\"\", []),\n",
    "(\"weird escape sequence\", \"\"\"\n",
    "'\\\\\\n\\\\''\n",
    "\"\"\", []),\n",
    "(\"long end of string escape\", \"\"\"\n",
    "'''\n",
    "# string\n",
    "\\\\'''\n",
    "# string\n",
    "'''\n",
    "\"\"\", []),\n",
    "(\"raw string escape\", \"\"\"\n",
    "r'''\n",
    "# string\n",
    "\\\\'''\n",
    "# string\n",
    "'''\n",
    "\"\"\", []),\n",
    "(\"multiple strings\", \"\"\"\n",
    "'''a''''''b'''\n",
    "\"\"\", []),\n",
    "]\n",
    "run_tests(test_strings, test_iter_comments, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class KeywordParser:\n",
    "    def __init__(self, *init_keywords):\n",
    "        self.parsers = {}\n",
    "        for kw in init_keywords: self.parsers[kw] = self._create_parser(kw)\n",
    "\n",
    "    def _create_parser(self, keyword):\n",
    "        # TODO: decide on the syntax\n",
    "        # TODO: Should there be any whitespace allowed before special comments?\n",
    "        # TODO: Should more than one \"#\" be allowed for special comments?\n",
    "        pattern = fr\"\"\"\n",
    "        ^              # start of line, since MULTILINE is passed\n",
    "        \\s*            # any amount of whitespace\n",
    "        \\#+\\s*          # literal \"#\", then any amount of whitespace\n",
    "        {keyword}(.*)  # keyword followed by arbitrary symbols (except new line)\n",
    "        $              # end of line, since MULTILINE is passed\n",
    "        \"\"\"\n",
    "        return re.compile(pattern, re.IGNORECASE | re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key in self.parsers: return self.parsers[key]\n",
    "        else:\n",
    "            parser = self._create_parser(key)\n",
    "            self.parsers[key] = parser\n",
    "            return parser\n",
    "        \n",
    "#     def search(self, key, text):\n",
    "#         return self[key].search('\\n'.join(detect_comments(text)))\n",
    "        \n",
    "#     def _search_remove(self, key, text):\n",
    "#         print('WARNING: _search_remove() DOESN\\'T WORK YET')\n",
    "#         # TODO: This function is supposed to remove the keyword comment from the input\n",
    "#         # TODO: detect_comments() has to be modified to allow for the positions to be returned\n",
    "#         parser = self[key]\n",
    "#         text, locations = detect_comments(text)\n",
    "#         for comment, l in zip(text, locations):\n",
    "#             res = parser.search(comment)\n",
    "#             if res: return res, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "OptionsTuple = namedtuple(typename='Options',\n",
    "                          field_names=['export_target', 'internal'],\n",
    "                          defaults=[None, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_re_legacy_options = re.compile(fr'^(i)?\\s*([a-zA-Z0-9]+\\S*|)\\s*$')\n",
    "def legacy_parse_options(options:str) -> OptionsTuple:\n",
    "    res = _re_legacy_options.search(options)\n",
    "    if res:\n",
    "        internal, export_target = res.groups()\n",
    "        return OptionsTuple(export_target=(export_target if export_target else None), internal=(internal == 'i'))\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parse_options(options:str, legacy:bool=True) -> OptionsTuple:\n",
    "    if (options is None) or (options == '') or (options.isspace()): return OptionsTuple()\n",
    "    else:\n",
    "        if legacy:\n",
    "            res = legacy_parse_options(options)\n",
    "            if res: return res\n",
    "        # TODO: New Syntax for specifying keyword options\n",
    "        raise NotImplementedError('this branch of parse_options() is not implemented yet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "keyword_parser = KeywordParser()\n",
    "def parse_export(source:str) -> (bool, OptionsTuple):\n",
    "    # TODO: This should check for all visibility affecting keywords, and prioritise the top most\n",
    "    #       That would allow the user to overwrite any unwanted cases\n",
    "    export, hide = keyword_parser['export'], keyword_parser['hide']\n",
    "    for comment, location in iter_comments(source):\n",
    "        res = export.search(comment)\n",
    "        if res: return (True, parse_options(res.groups()[0]))\n",
    "        res = hide.search(comment)\n",
    "        if res: return (False, None)\n",
    "    return (False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_exports(cells:list, default:str, code_only:bool=True) -> list:\n",
    "    # check for each cell if it's supposed to be exported and aggregate cell content together with export options\n",
    "    # remove whitespace at end of lines\n",
    "    exports = []\n",
    "    for i, cell in enumerate(cells):\n",
    "        if code_only and (cell.cell_type != 'code'): continue\n",
    "        else:\n",
    "            source = cell.source\n",
    "            to_export, options = parse_export(source)\n",
    "            if to_export:\n",
    "                assert options.export_target or default, f'Cell nr.{i} doesn\\'t have an export target, \\\n",
    "                                                           and a default is not specified:\\n{source}'\n",
    "                if not options.export_target: options = options._replace(export_target=default)\n",
    "                exports.append((source, options))\n",
    "            else: continue\n",
    "    return exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cell_type', 'metadata', 'source'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nb['cells'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell_type': 'markdown', 'metadata': {}, 'source': '# Imports'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nb['cells'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('# export\\nfrom collections import namedtuple, defaultdict\\nimport os\\nimport re\\nfrom nbdev.imports import *',\n",
       "  Options(export_target='export', internal=False)),\n",
       " ('#export\\ndef read_nb(fname):\\n    \"Read the notebook in `fname`.\"\\n    with open(Path(fname),\\'r\\', encoding=\\'utf8\\') as f: return nbformat.reads(f.read(), as_version=4)',\n",
       "  Options(export_target='export', internal=False)),\n",
       " ('# export\\nclass KeywordParser:\\n    def __init__(self, *init_keywords):\\n        self.parsers = {}\\n        for kw in init_keywords: self.parsers[kw] = self._create_parser(kw)\\n\\n    def _create_parser(self, keyword):\\n        # TODO: decide on the syntax\\n        # TODO: Should there be any whitespace allowed before special comments?\\n        # TODO: Should more than one \"#\" be allowed for special comments?\\n        pattern = fr\"\"\"\\n        ^              # start of line, since MULTILINE is passed\\n        \\\\s*            # any amount of whitespace\\n        \\\\#+\\\\s*          # literal \"#\", then any amount of whitespace\\n        {keyword}(.*)  # keyword followed by arbitrary symbols (except new line)\\n        $              # end of line, since MULTILINE is passed\\n        \"\"\"\\n        return re.compile(pattern, re.IGNORECASE | re.MULTILINE | re.VERBOSE)\\n\\n    def __getitem__(self, key):\\n        if key in self.parsers: return self.parsers[key]\\n        else:\\n            parser = self._create_parser(key)\\n            self.parsers[key] = parser\\n            return parser\\n        \\n#     def search(self, key, text):\\n#         return self[key].search(\\'\\\\n\\'.join(detect_comments(text)))\\n        \\n#     def _search_remove(self, key, text):\\n#         print(\\'WARNING: _search_remove() DOESN\\\\\\'T WORK YET\\')\\n#         # TODO: This function is supposed to remove the keyword comment from the input\\n#         # TODO: detect_comments() has to be modified to allow for the positions to be returned\\n#         parser = self[key]\\n#         text, locations = detect_comments(text)\\n#         for comment, l in zip(text, locations):\\n#             res = parser.search(comment)\\n#             if res: return res, l',\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\nOptionsTuple = namedtuple(typename='Options',\\n                          field_names=['export_target', 'internal'],\\n                          defaults=[None, False])\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\n_re_legacy_options = re.compile(fr'^(i)?\\\\s*([a-zA-Z0-9]+\\\\S*|)\\\\s*$')\\ndef legacy_parse_options(options:str) -> OptionsTuple:\\n    res = _re_legacy_options.search(options)\\n    if res:\\n        internal, export_target = res.groups()\\n        return OptionsTuple(export_target=(export_target if export_target else None), internal=(internal == 'i'))\\n    else: return None\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\ndef parse_options(options:str, legacy:bool=True) -> OptionsTuple:\\n    if (options is None) or (options == '') or (options.isspace()): return OptionsTuple()\\n    else:\\n        if legacy:\\n            res = legacy_parse_options(options)\\n            if res: return res\\n        # TODO: New Syntax for specifying keyword options\\n        raise NotImplementedError('this branch of parse_options() is not implemented yet.')\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\nkeyword_parser = KeywordParser()\\ndef parse_export(source:str) -> (bool, OptionsTuple):\\n    # TODO: This should check for all visibility affecting keywords, and prioritise the top most\\n    #       That would allow the user to overwrite any unwanted cases\\n    export, hide = keyword_parser['export'], keyword_parser['hide']\\n    for comment, location in iter_comments(source):\\n        res = export.search(comment)\\n        if res: return (True, parse_options(res.groups()[0]))\\n        res = hide.search(comment)\\n        if res: return (False, None)\\n    return (False, None)\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\ndef find_exports(cells:list, default:str, code_only:bool=True) -> list:\\n    # check for each cell if it's supposed to be exported and aggregate cell content together with export options\\n    # remove whitespace at end of lines\\n    exports = []\\n    for i, cell in enumerate(cells):\\n        if code_only and (cell.cell_type != 'code'): continue\\n        else:\\n            source = cell.source\\n            to_export, options = parse_export(source)\\n            if to_export:\\n                assert options.export_target or default, f'Cell nr.{i} doesn\\\\'t have an export target, \\\\\\n                                                           and a default is not specified:\\\\n{source}'\\n                if not options.export_target: options = options._replace(export_target=default)\\n                exports.append((source, options))\\n            else: continue\\n    return exports\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " ('# export\\nimport ast\\nfrom ast import iter_fields, AST\\nimport _ast\\nfrom pprint import pprint',\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\ndef remove_private_names(names):\\n    to_remove = {n for n in names if n.startswith('_')}\\n    return names.difference(to_remove)\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " ('# export\\ndef update_recursive(node, names=set()):\\n    \"\"\"inplace, recursive updating of names\"\"\"\\n    if isinstance(node, (_ast.List, _ast.Tuple)):\\n        for x in node.elts: update_recursive(x, names)\\n    elif isinstance(node, (_ast.Name)): names.add(node.id)\\n    elif isinstance(node, (_ast.Starred)): names.add(node.value.id)\\n    elif isinstance(node, (list, tuple)):\\n        for x in node: update_recursive(x, names)\\n    else: raise Exception(f\\'Couldn\\\\\\'t resolve {node} to name, unknown type\\')',\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\ndef parse_tree(tree):\\n    # TODO: find _all_ declarations\\n    names = set()\\n    for node in tree.body:\\n        node_name = node.__class__.__name__\\n        if   node_name == 'Assign'     : update_recursive(node.targets, names)\\n        elif node_name == 'FunctionDef': names.add(node.name)\\n        elif node_name == 'ClassDef'   : names.add(node.name)\\n        else: pass\\n    names = remove_private_names(names)\\n    return names\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " ('# export\\ndef find_names(code:str) -> list:\\n    # print(code)\\n    tree = ast.parse(code)\\n    names = parse_tree(tree)\\n    return names # modify ExportCache to accept a set()',\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\nclass ExportCache:\\n    def __init__(self, default_export=None):\\n        self.tupletype = namedtuple(typename='exports', field_names=['export_code', 'export_names'])\\n        self.exports = defaultdict(self._create_exp)\\n        if default_export is not None: self[default_export]\\n    \\n    def _create_exp(self): return self.tupletype(export_code=list(), export_names=set())\\n    \\n    def __getitem__(self, key): return self.exports[key]\\n    \\n    def add_names(self, key, names): self[key].export_names.update(names)\\n            \\n    def add_code(self, key, code): self[key].export_code.append(code)\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " (\"# export\\ndef find_default_export(cells:list) -> str:\\n    # search through all cells to find the default_exp keyword and return it's value.\\n    # syntax checking\\n    # maybe do some sanity checking\\n    return 'export'\\n    pass\",\n",
       "  Options(export_target='export', internal=False)),\n",
       " ('# export\\ndef create_mod_file(orig_nbfname, targ_pyfname):\\n    # create the .py file in the correct folder, with a header saying where it was originally from\\n    pass',\n",
       "  Options(export_target='export', internal=False)),\n",
       " ('# export\\ndef _notebook2script(cells=None, fname=None, silent=False, to_dict=False):\\n    \"\"\"Convert a single notebook\"\"\"\\n    # if cells: print(\\'WARNING: The Cells parameter is only used for testing purposes!\\')\\n    if fname is not None: raise NotImplementedError(\\'WARNING: fname is a \"must pass\", but not yet\\')\\n    # load notebook content\\n    # load config\\n    default = find_default_export(cells)\\n    if default is None:\\n        print(\\'WARNING: No default export file found! (should this crash, or see if each export has its own target?)\\')\\n    else:\\n        # maybe this should be done at the bottom, together with all the others\\n        # create_mod_file(original_nbfile_path, target_pyfile_path) # flipped in original code\\n        pass\\n    export_cache = ExportCache(default)\\n    # load _nbdev file and create a spec from it (no idea why this is needed)\\n    exports = find_exports(cells, default)\\n    for j, (code, options)  in enumerate(exports):\\n        # code = clean_code(code)\\n        e, i = options.export_target, options.internal\\n        if not i: export_cache.add_names(e, find_names(code))\\n        export_cache.add_code(e, code)\\n    # write_to_export_files(export_cache, default)\\n    # add names to _nbdev index\\n    # write code cell to file\\n    # save _nbdev file\\n    return export_cache',\n",
       "  Options(export_target='export', internal=False)),\n",
       " ('# export \\ndef notebook2script(fname=None, silent=False, to_dict=False):\\n    \"Convert notebooks matching `fname` to modules\"\\n    # initial checks\\n    if os.environ.get(\\'IN_TEST\\',0): return  # don\\'t export if running tests\\n    if fname is None:\\n        reset_nbdev_module()\\n        update_version()\\n        update_baseurl()\\n        files = [f for f in Config().nbs_path.glob(\\'*.ipynb\\') if not f.name.startswith(\\'_\\')]\\n    else: files = glob.glob(fname)\\n    d = collections.defaultdict(list) if to_dict else None\\n    for f in sorted(files): d = _notebook2script(f, silent=silent, to_dict=d)\\n    if to_dict: return d\\n    else: add_init(Config().lib_path)',\n",
       "  Options(export_target='export', internal=False))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_exports(test_nb['cells'], 'export', code_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- ALL TESTS COMPLETED ---------------\n",
      "9 / 9 Correct\n"
     ]
    }
   ],
   "source": [
    "test_strings = [\n",
    "(\"trippe quote(''')\", \"\"\"'''\n",
    "#export\n",
    "'''\"\"\", (False, None)),\n",
    "('tripple quote(\"\"\")', '''\"\"\"\n",
    "#export\n",
    "\"\"\"''', (False, None)),\n",
    "('single quote(\")', '\"\\\n",
    "\\n#export\\n\\\n",
    "\"', (False, None)),\n",
    "(\"single quote(')\", \"'\\\n",
    "\\n#export\\n\\\n",
    "'\", (False, None)),\n",
    "(\"correct\", \"\"\"\n",
    "#export\n",
    "\"\"\", (True, OptionsTuple())),\n",
    "(\"tricky case 1\", \"\"\"\n",
    "'this is a string'\n",
    "#export\n",
    "'this also, but between is an actual comment'\n",
    "\"\"\", (True, OptionsTuple())),\n",
    "(\"tricky case 2\", \"\"\"\n",
    "  a #export\n",
    "'''\n",
    "#export\n",
    "'''\n",
    "####export\"\"\", (True, OptionsTuple())),\n",
    "(\"tricky case 3\", \"\"\"\n",
    "'''\n",
    "'\n",
    "# export\n",
    "'\n",
    "'''\n",
    "\"\"\", (False, None)),\n",
    "(\"tricky case 4\", \"\"\"\n",
    "'''\n",
    "\\'\n",
    "# export\n",
    "\\'\n",
    "'''\n",
    "\"\"\", (False, None)),\n",
    "]\n",
    "run_tests(test_strings, parse_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- ALL TESTS COMPLETED ---------------\n",
      "8 / 11 Correct\n"
     ]
    }
   ],
   "source": [
    "test_markup = [\n",
    "('export', \"\"\"\n",
    "# export\n",
    "\"\"\", (True, OptionsTuple())),\n",
    "('comment layout', \"\"\"\n",
    "#export\n",
    "\"\"\", (True, OptionsTuple())),\n",
    "('export internal legacy', \"\"\"\n",
    "# exporti\n",
    "\"\"\", (True, OptionsTuple(internal=True))),\n",
    "('export internal', \"\"\"\n",
    "# export -i\n",
    "\"\"\", (True, OptionsTuple(internal=True))),\n",
    "('export show source', \"\"\"\n",
    "# export -s\n",
    "\"\"\", (True, OptionsTuple())),\n",
    "('export internal show', \"\"\"\n",
    "# export -i -s\n",
    "\"\"\", (True, OptionsTuple(internal=True))),\n",
    "('default empty', \"\"\"\n",
    "\n",
    "\"\"\", (False, None)),\n",
    "('hide', \"\"\"\n",
    "# hide\n",
    "\"\"\", (False, None)),\n",
    "('multiple comments', \"\"\"\n",
    "# export\n",
    "# hide\n",
    "\"\"\", (True, OptionsTuple())),\n",
    "('multi comment same line', \"\"\"\n",
    "# export hide\n",
    "\"\"\", (True, OptionsTuple(export_target='hide'))),\n",
    "('multiple comments default_exp', \"\"\"\n",
    "# export\n",
    "# default_exp\n",
    "\"\"\", (True, OptionsTuple())),\n",
    "]\n",
    "run_tests(test_markup, parse_export, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import ast\n",
    "from ast import iter_fields, AST\n",
    "import _ast\n",
    "# from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def print_tree(node):\n",
    "    if isinstance(node, (list, tuple)):\n",
    "        for x in node:\n",
    "            print_tree(x)\n",
    "    elif hasattr(node, '_fields'):\n",
    "        for f in node._fields:\n",
    "            # print(f)\n",
    "            print_tree(node.__getattribute__(f))\n",
    "    else:\n",
    "        print(node)\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "class TestParser(ast.NodeVisitor):\n",
    "    def visit(self, node):\n",
    "        method = 'visit_' + node.__class__.__name__\n",
    "        visitor = getattr(self, method, self.generic_visit)\n",
    "        print(f'{node.__class__.__name__} -> {visitor.__name__}')\n",
    "        return visitor(node)\n",
    "    \n",
    "    def _default(self, node):\n",
    "        # pprint(node.__dict__)\n",
    "        print(f'attr:   {node._attributes}\\nfields: {node._fields}\\n{\"-\"*25}')\n",
    "        \n",
    "    def visit_Assign(self, node): print(node.targets[0].id) # self._default(node)\n",
    "    \n",
    "    def visit_FunctionDef(self, node):\n",
    "        # self._default(node)\n",
    "        print(node.name)\n",
    "#         for d in node.decorator_list:\n",
    "#             print(self.visit(d))\n",
    "            \n",
    "    def visit_ClassDef(self, node): print(node.name) # self._default(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def remove_private_names(names):\n",
    "    to_remove = {n for n in names if n.startswith('_')}\n",
    "    return names.difference(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_attr(node:_ast.Attribute) -> str:\n",
    "    if isinstance(node.value, _ast.Attribute): return '.'.join((unwrap_attr(node.value), node.attr))\n",
    "    else: return '.'.join((node.value.id, node.attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def update_recursive(node, names):\n",
    "    \"\"\"inplace, recursive updating of names\"\"\"\n",
    "    if   isinstance(node, (_ast.List, _ast.Tuple)):\n",
    "        for x in node.elts: update_recursive(x, names)\n",
    "    elif isinstance(node, _ast.Name)   : names.append(node.id)\n",
    "    elif isinstance(node, _ast.Starred): names.append(node.value.id)\n",
    "    elif isinstance(node, _ast.Attribute) : names.append(unwrap_attr(node))\n",
    "    elif isinstance(node, list):\n",
    "        for x in node: update_recursive(x, names)\n",
    "    else: raise SyntaxError(f'Can\\'t resolve {node} to name, unknown type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_private(name): return not (name.startswith('_') and (not name.startswith('__')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_from_all_(node, names):\n",
    "    if isinstance(node, _ast.Str): names.add(node.s)\n",
    "    elif isinstance(node, _ast.Name): names.add(node.id)\n",
    "    elif isinstance(node, _ast.Attribute): names.add(unwrap_attr(node))\n",
    "    elif isinstance(node, (_ast.List, _ast.Tuple)):\n",
    "        for x in node.elts: update_from_all_(x, names)\n",
    "    elif isinstance(node, _ast.Starred): raise SyntaxError(f'Starred expression *{node.value.id} not allowed in _all_')\n",
    "    else: raise SyntaxError(f'{node} {node._attributes} {node._fields}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_names(node, names):\n",
    "    tmp_names = list()\n",
    "    update_recursive(node.targets, tmp_names)\n",
    "    print(tmp_names)\n",
    "    for name in tmp_names:\n",
    "        if not_private(name): names.add(name)\n",
    "        elif name == '_all_':\n",
    "            assert len(tmp_names) == 1, 'reserved keyword _all_ can only be used in simple assignments'\n",
    "            update_from_all_(node.value, out)\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_names(code:str) -> list:\n",
    "    tree = ast.parse(code) # @expensive\n",
    "    names = set()\n",
    "    for node in tree.body:\n",
    "        if   isinstance(node, _ast.Assign): add_names(node, names) # update_recursive(node.targets, names)\n",
    "        elif isinstance(node, (_ast.FunctionDef, _ast.ClassDef)) and not_private(node.name): names.add(node.name)\n",
    "        else: pass\n",
    "    # names = remove_private_names(names)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"_all_ = 'var_a'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"_all_ = ['var_a']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"_all_ = [*a]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"_all_ = ['var_a', var_b, a.b.c]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ast.parse(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = tree.body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('lineno', 'col_offset'), ('targets', 'value'))"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node._attributes, node._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_all_'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.targets[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Str' object has no attribute 'elts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-277-0af1da695b70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Str' object has no attribute 'elts'"
     ]
    }
   ],
   "source": [
    "node.value.elts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = node.value.elts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-261-81f819de85c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attributes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute '_attributes'"
     ]
    }
   ],
   "source": [
    "attr._attributes, attr._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a.b.c'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unwrap_attr(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ast.Name at 0x176042a0748>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr.value.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_ast.Attribute at 0x176042a07b8>, <_ast.Attribute at 0x176042a0e48>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr.value, attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_all_']\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_names(\"\"\"_all_ = ['var_a', var_b]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 / 10) TEST Default Assignment:\n",
      "['a']\n",
      "------------\n",
      "['b']\n",
      "------------\n",
      "['a']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(2 / 10) TEST Tuple unpacking:\n",
      "['a', 'b']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(3 / 10) TEST unpacking to tuples and lists:\n",
      "['a', 'b']\n",
      "------------\n",
      "['a', 'b']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(4 / 10) TEST unpacking to tuples and lists x2:\n",
      "['a', 'b']\n",
      "------------\n",
      "['a', 'b']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(5 / 10) TEST Multiple assignments:\n",
      "['a', 'b']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(6 / 10) TEST List Deconstruction:\n",
      "['head', 'tail']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(7 / 10) TEST Private Variables:\n",
      "['_a']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(8 / 10) TEST Dunder Variables:\n",
      "['__a']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(9 / 10) TEST Attribues:\n",
      "['a.b']\n",
      "------------\n",
      "TEST RESULT: SUCCESS\n",
      "\n",
      "(10 / 10) TEST _all_ special keyword:\n",
      "['_all_']\n",
      "------------\n",
      "TEST FAILED WITH EXCEPTION:\n",
      "TEST FAILED WITH RESULT: set()\n",
      "EXPECTED: {'var_a', 'c.d', 'a.b', 'var_b', '_abc'}\n",
      "\n",
      "--------------- ALL TESTS COMPLETED ---------------\n",
      "9 / 10 Correct\n"
     ]
    }
   ],
   "source": [
    "test_assignment = [\n",
    "('Default Assignment', \"\"\"\n",
    "a = 1\n",
    "b = a\n",
    "a = 2\n",
    "\"\"\", {'a', 'b'}),\n",
    "('Tuple unpacking', \"\"\"\n",
    "a, b = (1, 2)\n",
    "\"\"\", {'a', 'b'}),\n",
    "('unpacking to tuples and lists', \"\"\"\n",
    "(a, b) = (1, 2)\n",
    "[a, b] = (1, 2)\n",
    "\"\"\", {'a', 'b'}),\n",
    "('unpacking to tuples and lists x2', \"\"\"\n",
    "([a], (b)) = (1, 2)\n",
    "[[a, ((b))]] = (1, 2)\n",
    "\"\"\", {'a', 'b'}),\n",
    "('Multiple assignments', \"\"\"\n",
    "a = b = 2\n",
    "\"\"\", {'a', 'b'}),\n",
    "('List Deconstruction', \"\"\"\n",
    "head, *tail = [1,2,3,4,5]\n",
    "\"\"\", {'head', 'tail'}),\n",
    "('Private Variables', \"\"\"\n",
    "_a = 1\n",
    "\"\"\", set()),\n",
    "('Dunder Variables', \"\"\"\n",
    "__a = 1\n",
    "\"\"\", {'__a'}),\n",
    "('Attribues', \"\"\"\n",
    "a.b = 1\n",
    "\"\"\", {'a.b'}),\n",
    "('_all_ special keyword', \"\"\"\n",
    "_all_ = ['var_a', var_b, a.b, 'c.d', _abc]\n",
    "\"\"\", {'var_a', 'var_b', 'a.b', 'c.d', '_abc'}),\n",
    "]\n",
    "run_tests(test_assignment, find_names, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- ALL TESTS COMPLETED ---------------\n",
      "4 / 4 Correct\n"
     ]
    }
   ],
   "source": [
    "test_funcdef = [\n",
    "('Default function definition', \"\"\"\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\"\"\", {'add'}),\n",
    "('Type Annotated function def', \"\"\"\n",
    "def calc(a:int, b:int) -> int:\n",
    "    c:float = 2.0\n",
    "    return (a + b) * c\n",
    "\"\"\", {'calc'}),\n",
    "('function decorators', \"\"\"\n",
    "@test1\n",
    "@test2\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\"\"\", {'add'}),\n",
    "('@patch and more complex type annotations', \"\"\"\n",
    "@patch\n",
    "def func (obj:(Class1, Class2), a:int)->int:\n",
    "    pass\n",
    "\"\"\", {'func'})\n",
    "]\n",
    "run_tests(test_funcdef, find_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- ALL TESTS COMPLETED ---------------\n",
      "2 / 2 Correct\n"
     ]
    }
   ],
   "source": [
    "test_classdef = [\n",
    "('Default class definition', \"\"\"\n",
    "class Abc:\n",
    "    pass\n",
    "\"\"\", {'Abc'}),\n",
    "('Default class def 2', \"\"\"\n",
    "class Abc():\n",
    "    pass\n",
    "\"\"\", {'Abc'}),\n",
    "]\n",
    "run_tests(test_classdef, find_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ExportCache:\n",
    "    def __init__(self, default_export=None):\n",
    "        self.tupletype = namedtuple(typename='exports', field_names=['export_code', 'export_names'])\n",
    "        self.exports = defaultdict(self._create_exp)\n",
    "        if default_export is not None: self[default_export]\n",
    "    \n",
    "    def _create_exp(self): return self.tupletype(export_code=list(), export_names=set())\n",
    "    \n",
    "    def __getitem__(self, key): return self.exports[key]\n",
    "    \n",
    "    def add_names(self, key, names): self[key].export_names.update(names)\n",
    "            \n",
    "    def add_code(self, key, code): self[key].export_code.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_default_export(cells:list) -> str:\n",
    "    # search through all cells to find the default_exp keyword and return it's value.\n",
    "    # syntax checking\n",
    "    # maybe do some sanity checking\n",
    "    return 'export'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_mod_file(orig_nbfname, targ_pyfname):\n",
    "    # create the .py file in the correct folder, with a header saying where it was originally from\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _notebook2script(cells=None, fname=None, silent=False, to_dict=False):\n",
    "    \"\"\"Convert a single notebook\"\"\"\n",
    "    # if cells: print('WARNING: The Cells parameter is only used for testing purposes!')\n",
    "    if fname is not None: raise NotImplementedError('WARNING: fname is a \"must pass\", but not yet')\n",
    "    # load notebook content\n",
    "    # load config\n",
    "    default = find_default_export(cells)\n",
    "    if default is None:\n",
    "        print('WARNING: No default export file found! (should this crash, or see if each export has its own target?)')\n",
    "    else:\n",
    "        # maybe this should be done at the bottom, together with all the others\n",
    "        # create_mod_file(original_nbfile_path, target_pyfile_path) # flipped in original code\n",
    "        pass\n",
    "    export_cache = ExportCache(default)\n",
    "    # load _nbdev file and create a spec from it (no idea why this is needed)\n",
    "    exports = find_exports(cells, default)\n",
    "    for j, (code, options)  in enumerate(exports):\n",
    "        # code = clean_code(code)\n",
    "        e, i = options.export_target, options.internal\n",
    "        if not i: export_cache.add_names(e, find_names(code))\n",
    "        export_cache.add_code(e, code)\n",
    "    # write_to_export_files(export_cache, default)\n",
    "    # add names to _nbdev index\n",
    "    # write code cell to file\n",
    "    # save _nbdev file\n",
    "    return export_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%%prun\n",
    "ec = _notebook2script(test_nb['cells'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# export\\nfrom collections import namedtuple, defaultdict',\n",
      " '# export\\nimport os',\n",
      " '# export\\nimport re',\n",
      " '# export\\n'\n",
      " 'def run_tests(cases, func, verbose=False):\\n'\n",
      " '    nr_correct = 0\\n'\n",
      " '    for i, (n, c, r) in enumerate(cases):\\n'\n",
      " \"        if verbose: print(f'({i + 1} / {len(cases)}) TEST {n}:')\\n\"\n",
      " '        try:\\n'\n",
      " '            res = func(c)\\n'\n",
      " \"            assert res == r, f'TEST FAILED WITH RESULT: {res}\\\\nEXPECTED: \"\n",
      " \"{r}'\\n\"\n",
      " '            nr_correct += (res == r)\\n'\n",
      " \"            if verbose: print(f'TEST RESULT: SUCCESS\\\\n')\\n\"\n",
      " '        except Exception as e:\\n'\n",
      " \"            if verbose: print(f'TEST FAILED WITH EXCEPTION:\\\\n{e}\\\\n')\\n\"\n",
      " '            # raise e\\n'\n",
      " \"    print('--------------- ALL TESTS COMPLETED ---------------')\\n\"\n",
      " \"    print(f'{nr_correct} / {len(cases)} Correct')\",\n",
      " '# export\\nfrom nbdev.imports import *',\n",
      " '#export\\n'\n",
      " 'def read_nb(fname):\\n'\n",
      " '    \"Read the notebook in `fname`.\"\\n'\n",
      " \"    with open(Path(fname),'r', encoding='utf8') as f: return \"\n",
      " 'nbformat.reads(f.read(), as_version=4)',\n",
      " '# export\\n'\n",
      " 'def detect_comments(s:str, pure_comments_only:bool=True):\\n'\n",
      " '    # TODO: should non-pure comments ever be allowed?\\n'\n",
      " '    # TODO: are locations needed?\\n'\n",
      " \"    in_str, str_enter = False, ''\\n\"\n",
      " '    escape = False\\n'\n",
      " '    comments = list()\\n'\n",
      " '    # locations = list()\\n'\n",
      " '    for i, line in enumerate(s.splitlines()):\\n'\n",
      " '        is_pure_comment = True\\n'\n",
      " '        for j, char in enumerate(line):\\n'\n",
      " \"            if not (char.isspace() or char == '#'): is_pure_comment = False\\n\"\n",
      " '            if in_str:\\n'\n",
      " '                if (char == str_enter): in_str = False\\n'\n",
      " '            else:\\n'\n",
      " \"                if (char == '#'):\\n\"\n",
      " '                    if pure_comments_only:\\n'\n",
      " '                        if is_pure_comment: comments.append(line)# ; '\n",
      " 'locations.append((i, 0))\\n'\n",
      " '                    else: comments.append(line[j:])# ; locations.append((i, '\n",
      " 'j))\\n'\n",
      " '                    break\\n'\n",
      " '                elif char == \"\\'\": in_str, str_enter = True, \"\\'\"\\n'\n",
      " '                elif char == \\'\"\\': in_str, str_enter = True, \\'\"\\'\\n'\n",
      " '    return comments# , locations',\n",
      " '# export\\n'\n",
      " 'class KeywordParser:\\n'\n",
      " '    def __init__(self, *init_keywords):\\n'\n",
      " '        self.parsers = {}\\n'\n",
      " '        for kw in init_keywords: self.parsers[kw] = self._create_parser(kw)\\n'\n",
      " '\\n'\n",
      " '    def _create_parser(self, keyword):\\n'\n",
      " '        # TODO: Tighten down the syntax\\n'\n",
      " '        # TODO: Should there be any whitespace allowed before special '\n",
      " 'comments?\\n'\n",
      " '        # TODO: Should more than one \"#\" be allowed for special comments?\\n'\n",
      " '        pattern = fr\"\"\"\\n'\n",
      " '        ^              # start of line, since MULTILINE is passed\\n'\n",
      " '        \\\\s*            # any amount of whitespace\\n'\n",
      " '        \\\\#+\\\\s*          # literal \"#\", then any amount of whitespace\\n'\n",
      " '        {keyword}(.*)  # keyword followed by arbitrary symbols (except new '\n",
      " 'line)\\n'\n",
      " '        $              # end of line, since MULTILINE is passed\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        return re.compile(pattern, re.IGNORECASE | re.MULTILINE | '\n",
      " 're.VERBOSE)\\n'\n",
      " '\\n'\n",
      " '    def __getitem__(self, key):\\n'\n",
      " '        if key in self.parsers: return self.parsers[key]\\n'\n",
      " '        else:\\n'\n",
      " '            parser = self._create_parser(key)\\n'\n",
      " '            self.parsers[key] = parser\\n'\n",
      " '            return parser\\n'\n",
      " '        \\n'\n",
      " '    def search(self, key, text):\\n'\n",
      " \"        return self[key].search('\\\\n'.join(detect_comments(text)))\\n\"\n",
      " '        \\n'\n",
      " '    def _search_remove(self, key, text):\\n'\n",
      " \"        print('WARNING: _search_remove() DOESN\\\\'T WORK YET')\\n\"\n",
      " '        # TODO: This function is supposed to remove the keyword comment from '\n",
      " 'the input\\n'\n",
      " '        # TODO: detect_comments() has to be modified to allow for the '\n",
      " 'positions to be returned\\n'\n",
      " '        parser = self[key]\\n'\n",
      " '        text, locations = detect_comments(text)\\n'\n",
      " '        for comment, l in zip(text, locations):\\n'\n",
      " '            res = parser.search(comment)\\n'\n",
      " '            if res: return res, l',\n",
      " '# export\\n'\n",
      " \"OptionsTuple = namedtuple(typename='Options',\\n\"\n",
      " \"                          field_names=['export_target', 'internal'],\\n\"\n",
      " '                          defaults=[None, False])',\n",
      " '# export\\n'\n",
      " \"_re_legacy_options = re.compile(fr'^(i)?\\\\s*([a-zA-Z0-9]+\\\\S*|)\\\\s*$')\\n\"\n",
      " 'def legacy_parse_options(options:str) -> OptionsTuple:\\n'\n",
      " '    res = _re_legacy_options.search(options)\\n'\n",
      " '    if res:\\n'\n",
      " '        internal, export_target = res.groups()\\n'\n",
      " '        return OptionsTuple(export_target=(export_target if export_target '\n",
      " \"else None), internal=(internal == 'i'))\\n\"\n",
      " '    else: return None',\n",
      " '# export\\n'\n",
      " 'def parse_options(options:str, legacy:bool=True) -> OptionsTuple:\\n'\n",
      " \"    if (options is None) or (options == '') or (options.isspace()): return \"\n",
      " 'OptionsTuple()\\n'\n",
      " '    else:\\n'\n",
      " '        if legacy:\\n'\n",
      " '            res = legacy_parse_options(options)\\n'\n",
      " '            if res: return res\\n'\n",
      " '        # TODO: New Syntax for specifying keyword options\\n'\n",
      " \"        raise NotImplementedError('this branch of parse_options() is not \"\n",
      " \"implemented yet.')\",\n",
      " '# export\\n'\n",
      " 'keyword_parser = KeywordParser()\\n'\n",
      " 'def parse_export(source:str) -> (bool, OptionsTuple):\\n'\n",
      " '    # TODO: This should check for all visibility affecting keywords, and '\n",
      " 'prioritise the top most\\n'\n",
      " '    #       That would allow the user to overwrite any unwanted cases\\n'\n",
      " \"    res = keyword_parser.search('export', source)\\n\"\n",
      " '    if res: return (True, parse_options(res.groups()[0]))\\n'\n",
      " '    else: return (False, None)',\n",
      " '# export\\n'\n",
      " 'def find_exports(cells:list, default:str, code_only:bool=True) -> list:\\n'\n",
      " \"    # check for each cell if it's supposed to be exported and aggregate cell \"\n",
      " 'content together with export options\\n'\n",
      " '    # remove whitespace at end of lines\\n'\n",
      " '    exports = []\\n'\n",
      " '    for i, cell in enumerate(cells):\\n'\n",
      " \"        if code_only and (cell.cell_type != 'code'): continue\\n\"\n",
      " '        else:\\n'\n",
      " '            source = cell.source\\n'\n",
      " '            to_export, options = parse_export(source)\\n'\n",
      " '            if to_export:\\n'\n",
      " \"                assert options.export_target or default, f'Cell nr.{i} \"\n",
      " \"doesn\\\\'t have an export target, \\\\\\n\"\n",
      " '                                                           and a default is '\n",
      " \"not specified:\\\\n{source}'\\n\"\n",
      " '                if not options.export_target: options = '\n",
      " 'options._replace(export_target=default)\\n'\n",
      " '                exports.append((source, options))\\n'\n",
      " '            else: continue\\n'\n",
      " '    return exports',\n",
      " '# export\\n'\n",
      " 'import ast\\n'\n",
      " 'from ast import iter_fields, AST\\n'\n",
      " 'import _ast\\n'\n",
      " 'from pprint import pprint',\n",
      " '# export\\n'\n",
      " 'def remove_private_names(names):\\n'\n",
      " \"    to_remove = {n for n in names if n.startswith('_')}\\n\"\n",
      " '    return names.difference(to_remove)',\n",
      " '# export\\n'\n",
      " 'def update_recursive(node, names=set()):\\n'\n",
      " '    \"\"\"inplace, recursive updating of names\"\"\"\\n'\n",
      " '    if isinstance(node, (_ast.List, _ast.Tuple)):\\n'\n",
      " '        for x in node.elts: update_recursive(x, names)\\n'\n",
      " '    elif isinstance(node, (_ast.Name)): names.add(node.id)\\n'\n",
      " '    elif isinstance(node, (_ast.Starred)): names.add(node.value.id)\\n'\n",
      " '    elif isinstance(node, (list, tuple)):\\n'\n",
      " '        for x in node: update_recursive(x, names)\\n'\n",
      " \"    else: raise Exception(f'Couldn\\\\'t resolve {node} to name, unknown \"\n",
      " \"type')\",\n",
      " '# export\\n'\n",
      " 'def parse_tree(tree):\\n'\n",
      " '    # TODO: find _all_ declarations\\n'\n",
      " '    names = set()\\n'\n",
      " '    for node in tree.body:\\n'\n",
      " '        node_name = node.__class__.__name__\\n'\n",
      " \"        if   node_name == 'Assign'     : update_recursive(node.targets, \"\n",
      " 'names)\\n'\n",
      " \"        elif node_name == 'FunctionDef': names.add(node.name)\\n\"\n",
      " \"        elif node_name == 'ClassDef'   : names.add(node.name)\\n\"\n",
      " '        else: pass\\n'\n",
      " '    names = remove_private_names(names)\\n'\n",
      " '    return names',\n",
      " '# export\\n'\n",
      " 'def find_names(code:str) -> list:\\n'\n",
      " '    # print(code)\\n'\n",
      " '    tree = ast.parse(code)\\n'\n",
      " '    names = parse_tree(tree)\\n'\n",
      " '    return names # modify ExportCache to accept a set()',\n",
      " '# export\\n'\n",
      " 'class ExportCache:\\n'\n",
      " '    def __init__(self, default_export=None):\\n'\n",
      " \"        self.tupletype = namedtuple(typename='exports', \"\n",
      " \"field_names=['export_code', 'export_names'])\\n\"\n",
      " '        self.exports = defaultdict(self._create_exp)\\n'\n",
      " '        if default_export is not None: self[default_export]\\n'\n",
      " '    \\n'\n",
      " '    def _create_exp(self): return self.tupletype(export_code=list(), '\n",
      " 'export_names=set())\\n'\n",
      " '    \\n'\n",
      " '    def __getitem__(self, key): return self.exports[key]\\n'\n",
      " '    \\n'\n",
      " '    def add_names(self, key, names): self[key].export_names.update(names)\\n'\n",
      " '            \\n'\n",
      " '    def add_code(self, key, code): self[key].export_code.append(code)',\n",
      " '# export\\n'\n",
      " 'def find_default_export(cells:list) -> str:\\n'\n",
      " '    # search through all cells to find the default_exp keyword and return '\n",
      " \"it's value.\\n\"\n",
      " '    # syntax checking\\n'\n",
      " '    # maybe do some sanity checking\\n'\n",
      " \"    return 'export'\\n\"\n",
      " '    pass',\n",
      " '# export\\n'\n",
      " 'def create_mod_file(orig_nbfname, targ_pyfname):\\n'\n",
      " '    # create the .py file in the correct folder, with a header saying where '\n",
      " 'it was originally from\\n'\n",
      " '    pass',\n",
      " '# export\\n'\n",
      " 'def _notebook2script(cells=None, fname=None, silent=False, to_dict=False):\\n'\n",
      " '    \"\"\"Convert a single notebook\"\"\"\\n'\n",
      " \"    if cells: print('WARNING: The Cells parameter is only used for testing \"\n",
      " \"purposes!')\\n\"\n",
      " \"    if fname is not None: raise NotImplementedError('WARNING: fname is a \"\n",
      " '\"must pass\", but not yet\\')\\n'\n",
      " '    # load notebook content\\n'\n",
      " '    # load config\\n'\n",
      " '    default = find_default_export(cells)\\n'\n",
      " '    if default is None:\\n'\n",
      " \"        print('WARNING: No default export file found! (should this crash, or \"\n",
      " \"see if each export has its own target?)')\\n\"\n",
      " '    else:\\n'\n",
      " '        # maybe this should be done at the bottom, together with all the '\n",
      " 'others\\n'\n",
      " '        # create_mod_file(original_nbfile_path, target_pyfile_path) # '\n",
      " 'flipped in original code\\n'\n",
      " '        pass\\n'\n",
      " '    export_cache = ExportCache(default)\\n'\n",
      " '    # load _nbdev file and create a spec from it (no idea why this is '\n",
      " 'needed)\\n'\n",
      " '    exports = find_exports(cells, default)\\n'\n",
      " '    for j, (code, options)  in enumerate(exports):\\n'\n",
      " '        # code = clean_code(code)\\n'\n",
      " '        e, i = options.export_target, options.internal\\n'\n",
      " '        if not i: export_cache.add_names(e, find_names(code))\\n'\n",
      " '        export_cache.add_code(e, code)\\n'\n",
      " '    # write_to_export_files(export_cache, default)\\n'\n",
      " '    # add names to _nbdev index\\n'\n",
      " '    # write code cell to file\\n'\n",
      " '    # save _nbdev file\\n'\n",
      " '    return export_cache',\n",
      " '# export \\n'\n",
      " 'def notebook2script(fname=None, silent=False, to_dict=False):\\n'\n",
      " '    \"Convert notebooks matching `fname` to modules\"\\n'\n",
      " '    # initial checks\\n'\n",
      " \"    if os.environ.get('IN_TEST',0): return  # don't export if running tests\\n\"\n",
      " '    if fname is None:\\n'\n",
      " '        reset_nbdev_module()\\n'\n",
      " '        update_version()\\n'\n",
      " '        update_baseurl()\\n'\n",
      " \"        files = [f for f in Config().nbs_path.glob('*.ipynb') if not \"\n",
      " \"f.name.startswith('_')]\\n\"\n",
      " '    else: files = glob.glob(fname)\\n'\n",
      " '    d = collections.defaultdict(list) if to_dict else None\\n'\n",
      " '    for f in sorted(files): d = _notebook2script(f, silent=silent, '\n",
      " 'to_dict=d)\\n'\n",
      " '    if to_dict: return d\\n'\n",
      " '    else: add_init(Config().lib_path)']\n",
      "\n",
      "{'parse_export', 'update_recursive', 'parse_tree', 'create_mod_file', 'read_nb', 'OptionsTuple', 'run_tests', 'legacy_parse_options', 'parse_options', 'ExportCache', 'detect_comments', 'find_exports', 'KeywordParser', 'remove_private_names', 'find_names', 'keyword_parser', 'find_default_export', 'notebook2script'}\n"
     ]
    }
   ],
   "source": [
    "for key in ec.exports.keys():\n",
    "    exp = ec.exports[key]\n",
    "    pprint(exp.export_code)\n",
    "    print('')\n",
    "    print(exp.export_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def notebook2script(fname=None, silent=False, to_dict=False):\n",
    "    \"Convert notebooks matching `fname` to modules\"\n",
    "    # initial checks\n",
    "    if os.environ.get('IN_TEST',0): return  # don't export if running tests\n",
    "    if fname is None:\n",
    "        reset_nbdev_module()\n",
    "        update_version()\n",
    "        update_baseurl()\n",
    "        files = [f for f in Config().nbs_path.glob('*.ipynb') if not f.name.startswith('_')]\n",
    "    else: files = glob.glob(fname)\n",
    "    d = collections.defaultdict(list) if to_dict else None\n",
    "    for f in sorted(files): d = _notebook2script(f, silent=silent, to_dict=d)\n",
    "    if to_dict: return d\n",
    "    else: add_init(Config().lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
